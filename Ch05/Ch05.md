	
## Counters and Time Series in RavenDB

The previous chapter covered the RavenDB client API and focused on using RavenDB as a document database. This is the 
most common usage scenario for RavenDB, but it isn't the only one. We looked into attachments
in the previous chapter, as a way to add binary data to documents. In this chapter, we are going to talk about using
RavenDB to store counters and timeseries.

Like attachments, counters and time series are strongly tied to a document. Unlike attachments, we aren't talking about
just storing binary data and calling a day. With both counters and time series, RavenDB support complex operations and 
the ability to understand and work with the data in non trivial ways.

But before we get to the gist of these features, let's talk about why does RavenDB even need to have dedicated features
such as these? Surely we can just store the data in the document itself and call it a day, no?

Consider the document fragment shown in Figure 5.1, in the image, you can see how we can store counters and time series
data directly in the document. So why do we need a dedicated feature for something that is already there?

![Document fragment storing counters and time series data](./Ch05/img01.png)

The answer is quite simple. If the data is inside the document, any change to the data means changing the document as a
whole. Consider the case of the `DownloadsCount` that we have in Figure 5.1. If we need to update it whenever a user 
click on a download link, what are the implications?

We can load the document, increment the count and save it back. This works, unless there are two users who click on the 
link at the same time. We can use a `Patch` operation to avoid concurrency issues, of course, but at the database's side
of things, RavenDB will still need to load/modify/store the document. 

Another important aspect is what happens when we have _other_ changes to the document while we are incrementing the 
value. Similar concerns apply to the `DailyDownloads` field as well. But in addition to the concurrency aspect of 
counters, with time series we have to take into account that the _size_ of the data might grow very big. To the point
where most of the document's data would be the time series data. And while we certainly want to keep track of that, we 
don't want to keep it in the same place as the document itself.

We have talked about different ways to handle that, with splitting the logical document into separate physical 
documents in the previous chapter. But a much better option is to have support for these scenarios directly inside 
RavenDB, which is why we have these features.

### Counters

Counters in RavenDB are pretty simple. They are just named signed 64 bits values that can be attached to a document. 
At first glance, they don't look like much. But the concept of counting is so pervasive that it turns out that they 
are quite useful in many scenarios. 

Figure 5.2 shows a counter in the Studio. As you can see, this is quite similar to the way you would use this value if
it was embedded inside the document. The only thing we have done so far is just move it aside a bit. 

![A document counter in the Studio](./Ch05/img02.png)

Moving the counter to the side is actually the whole point. Because now we can treat the counter in several special 
ways. To start with, changing the counter does _not_ require us to change the document. This means that when we want
to increment or decrement the value, we don't need to load / modify / store the document. That reduce the cost of 
changing the value of the counter. 

#### Writing counter values

Updating a counter value is cheap, _very_ cheap. This fact allows us to nicely handle many interesting cases. 
Consider the scenario of two users clicking on the download link at the same time. Listing 5.1 has the code that 
will increment the number of downloads for the document.

```{caption="Incrementing the number of downloads of a document" .cs}
using (var session = store.OpenSession())
{
    session.CountersFor("packages/393-A")
    	.Increment("DownloadsCount", delta: 1);
    session.SaveChanges();
}
```

The code in Listing 5.1 has a few interesting implications. First, as usual, a change to counter is part of the session
and has the same transactional guarantees as other operations in RavenDB. You can have a document change and a counter
increment (to the same document or to a different document) that will all be part of the same transaction.
Second, concurrent operations on the counter are safe to perform and will simply resolve to the appropriate value after 
taking into account all the operations on that particular counter. Third, we don't need to create our counter ahead
of time. The first operation on a counter on a document will create the (zero initialized) counter. 
This reduce the amount of work you have to deal with when working with counters.

> **Counters are natively distributed**
> 
> Counters not only handle concurrent modifications to the counter safely and correctly, they also handle the problem
> of _distributed_ concurrent modifications. Two clients may update the same counter on the same document 
> on two separate nodes in a  RavenDB cluster, and RavenDB will know how to resolve all those writes to get 
> to the right value in the end.
>
> RavenDB uses a data structure called CRDT^[Conflict-free replicated data type - in particular, RavenDB uses a 
> variant of the PN-Counter data type.] for this purpose, 
> which allows us to merge multiple writes and reach the correct tally without the need for synchronization 
> or complexity on the part of the users. 
>
> Under the covers, RavenDB maintains a value per each node the counter was modified on. So we can merge all those
> values together to get the final tally for the counter, without having to coordinate in a distributed system.

These features make RavenDB counters highly suitable for tasks that require high level of concurrency in counting. The 
example of `DownloadsCount` is a good representation of the kind of usage you'll get from counters. Counters are used
for capturing ad impressions, page views and tracking the number of API calls across your system.

> **Decrementing a counter value**
> 
> RavenDB doesn't provide an API call to `Decrement` a value. You can simply pass a negative value to `Increment`
> to reduce the counter value.


#### Reading counter values

After looking at writing counter values, let's see how we can read them. In Listing 5.2 you can _read_ counters values.

```{caption="Reading a counter value via the Session API" .cs}
using (var session = store.OpenSession())
{
    long? downloads = session.CountersFor("packages/393-A")
    	.Get("DownloadsCounter");
}
```

The API is fairly simple, all you need to state is what document you are operating on and what counter's value you want
to get. You get the value back as a 64 bits number, and that is pretty much it. Well, almost.

In order to get the counter value, you need to make a remote call to the server. In Chapter 2, we talked about the 
high costs of remote calls and how RavenDB has the notion of Includes specifically to avoid these costs. Counters can
also be included, as you can see in Listing 5.3.

```{caption="Including a counter value during document load to reduce remote calls" .cs}
using (var session = store.OpenSession())
{
	var package = session.Load<Package>(
		"packages/393-A", 
		i => i.IncludeCounter("DownloadsCount")
	);
    long? downloads = session.CountersFor("packages/393-A")
    	.Get("DownloadsCounter");
}
```

The `Get` call in Listing 5.3 will not trigger a remote call to the server. The data has already been been gathered by
the session in the `Load` call, because we explicitly asked for it. The RavenDB Client API has various overloads to 
get all the counters for a document, a specific counter or a set of them. The end result in all cases is that you can
reduce the number of round trips that you have to pay for and gain better overall performance.

> **Including counters in queries**
> 
> We have barely talked about queries so far (we'll do so quite extensively in the 3rd part of this book). But as a 
> sneak peak, you can also include counters during queries, like so: `from Packages include counters('DownloadsCount')`.
> And you can get all the counters for a document using: `from Packages include counters()`.

#### Counters metadata and data modeling

Counters are stored outside of the document itself, as we previously discussed. However, the document itself does 
contain some information about the counters. Most importantly, the document's metadata contains two important
properties relating to counters, as you can see in Figure 5.3.

![Looking at counters metadata in the studio](./Ch05/img03.png)

The first property of interest is the `@flags` property. You can see that in the case of Figure 5.3, its value is 
`HasCounters`. The `@flags` property is used by RavenDB to surface a lot of the internal state of the document and
you can make use of it in your code to make decisions about the document.

The second property of interest here is the `@counters` property. This is a metadata property that holds an array 
of all the counters names that exists on this document. In other words, merely the act of loading a document 
will give you access to all the counter names that this document possesses. If the only thing you use counters for
are for things like download metrics, the `@counters` property isn't particularly interesting. 

There are far more interesting scenarios for using counters, however. To the point where we need to talk about 
data modeling with counters. On initial consideration, data modeling with counters seems... odd. There isn't much
to _do_ with counters. Each counter represent a signed 64 bits number, which can range between  
`-9,223,372,036,854,775,808` to positive `9,223,372,036,854,775,807`. What kind of data modeling do we have in this case?

The data modeling part isn't so much for the values, but for the counters _names_. Consider a scenario where I want
to track downloads not just globally, but also by location. I can use a `DownloadsCount` to track the global number
but also have `Downloads/EU` and `Downloads/USA` as counters and increment them as well, based on the source of the 
download. 

Another good example would be to track the number of requests to a 3rd party service. We may want to keep track not 
just on the number of requests, but also their results. Figure 5.4 shows a simple way to handle this task.

![Tracking request HTTP statuses via counters](./Ch05/img04.png)

The advantage here is that counters are small and efficient, designed to be used in scenarios that demand high 
throughput. Using them to track high frequency events, and using different counters to track different statuses of 
the event can simplify such tasks greatly.

> **How many counters?**
>
> A document can have any number of counters attached to it. We have tested documents that had hundreds of thousands
> of counters, so there isn't a _hard_ limit. However, you should be aware of practical realities. 
>
> The names of counters on a document are stored in the document metadata. If you have a document that have a _lot_
> of counters, the size of the document's counters' metadata might grow to be a significant portion of the
> document size. That is not a desired scenario. Things would still work, but you can do better by
> splitting the data to multiple physical documents at that point.

You can also use counters in queries, as you can see in Listing 5.4.

```{caption="Getting the values of counters during queries" .sql}
from Services as service
select service.Name, 
	counter(service, "status/500") as Errors
```

We'll cover queries in depth in Part III of this book, so I won't go over these in detail. But you can probably already
see how useful such a feature can be. In this case, you don't need any special API calls, the resulting value is sent
as a simple property of the result set of the query.

#### Consistency properties of counters

Counters in RavenDB are resilient to concurrency and distribution issues. You can increment or decrement a counter 
at the same time on multiple nodes that are wildly separated from one another and RavenDB will gather all the 
operations on the counter and reconcile them to a single final value. 

RavenDB ensures the ability to mutate counter values concurrently in distributed environment for existing counters as
well as new counters. For existing counters, we need to merge the disparate counter values. Changing a counter value,
after all, does not change the document. That is the whole _point_ of counters.

Creating a new counter, however, does change the document. In particular, the name of the new counter is written to
the document metadata. That means that two writes to separate counters on the same documents on different machines will
create (slightly) different documents. RavenDB is aware of this and will resolve conflicts on the `@counters` property
automatically for you.

The fact that you can freely mutate or create a counter in a distributed environment without care is valuable 
and useful in many scenarios, but it comes at a cost.

RavenDB guarantees that writes to counters are ACID. That is, if you make a write to a counter, we'll accept the write
and persist it. Even in the presence of failure. If the counter write is part of a larger transaction (such as 
modifying multiple counters and / or documents), we also guarantee that the whole transaction (including the counter 
writes) is committed or rolled back as a whole. 

However, the counter value itself is using an eventual consistency mode. This is because even though you might have
incremented the value in a transaction (and gotten the modified value back), you can't be sure that the value hasn't
also been modified in another node. Eventually that other write will reach all the nodes in the cluster, but certain
failure modes can cause it to take quite a while. 

If you need to _rely_ on the counter value for operational reasons, you might want to consider either a property 
inside the document or use a compare exchange value, as we discussed in Chapter 2. Most usages of counters in 
RavenDB are resilient to the eventual consistency inherit to using them.

#### Deleting counters

Counters can be deleted using the Studio or the RavenDB Client API, as you can see in Listing 5.5. 

```{caption="Deleting a counter from a document" .cs}
using (var session = store.OpenSession())
{
    session.CountersFor("packages/393-A").Delete("DownloadsCounter");
    session.SaveChanges();
}
```

A counter deletion will be sent to the server when `SaveChanges` is called, as expected. One thing to note about the 
act of deleting a counter. Like all other operations on counters, we need to consider what will happen when there are
concurrent operations. One for deleting a counter and another to increment it. In this case, a lot depend on the actual
order of operations.

If the increment operation runs first, the counter will be deleted. However, if the delete happens to win the race, the
counter will be deleted, and then recreated by the increment operation. In a distributed setting, a delete on one node
and an update on another node will always resolve in favor of the update. 

If you need to delete counters, the usual way to do so it to ensure that in your application,
you aren't going to also try to write to them at the same time. 
But for the most part, counter deletions are relatively rare operation for typical applications. 

It is common to want to _reset_ a counter to zero, however. For example, if you are tracking services' status codes, 
you might want to reset the counter whenever a new version of the service has been deployed. This is usually better
to handle explicitly, as you can see in Listing 5.6.

```{caption="Resetting a counter value" .cs}
using (var session = store.OpenSession())
{
	var serviceCounters = session.CountersFor("services/2338-C");
	long failures = serviceCounters.Get("status/500") ?? 0;
    serviceCounters.Increment("status/500", -failures);
    session.SaveChanges();
}
```

In Listing 5.6, we reset the counter value to zero by getting the value and decrementing the current value. Other 
operations that might be modifying the counter at the same time can run concurrently without any issue.

Alternatively, you might have a different counter for each version of the service. So when you deploy a new version
your code will start incrementing the `status/3.2/500` counter and no longer modify the `status/3.1/500`
counter, which an now be safely deleted. This is a simple example of how you can ensure that there won't be concurrent
delete and increment operations on a counter.

#### Deleting a document with counters

A counter is attached to a document, and the deletion of a document will delete all the counters of a document. This 
might seems obvious, but I wanted to call it out explicitly. Deleting a document and then creating a new document
with the same name isn't going to restore the old counters.

In the same manner, an increment operation on a counter will create the counter if it doesn't already exists. However,
an increment operation on a counter of a _document_ that doesn't exists will fail. Counters can be created on the fly
but the documents that they belong to must already exist beforehand.

#### Patching counters

In Chapter 4, we talked about patching documents. The patching feature allow you to run a script that will run at 
the server side and modify documents based on your own logic. Patch operations also allow you to modify counters and
their values. 

For example, let's assume that we have an existing document with some properties that I want to turn into counters.
The code in Listing 5.7 shows how this can be done.

```{caption="Creating a counter from a document property, using patching" .cs}
using (var session = store.OpenSession())
{
	var script = @"
if (this.DownloadCount !== undefined){
	incrementCounter(this, "DownloadCount", 
		this.DownloadCount);
	delete this.DownloadCount;
}";
	session.Advanced.Defer(new PatchCommandData(
	    id: "packages/393-A",
	    changeVector: null,
	    patch: new PatchRequest
	    {
	        Script = script
	    }, patchIfMissing: null));
    session.SaveChanges();
}
```

The only items of interest in the patch in Listing 5.7 is the call to `incrementCounter`, which will create the counter
if it doesn't already exists. The rest of the code is identical to the patch operations that we already covered in 
Chapter 4. 

The patch API provides a few methods to work with counters:

* `incrementCounter(document, counterName, value);`
* `counter(document, counterName);`
* `deleteCounter(document, counterName);`

If you want to get the list of counters on a document, you can use the metadata property `@counters` to get the list
of all the counter names for a document.

You can use these methods and the `@counters` metadata property to programmatically modify documents and their counters
freely. 

#### Querying on counter values

Counters in RavenDB are meant for high frequency updates. Updating a counter value is something that is expected to happen
a _lot_. As such, RavenDB minimize the amount of work that is involved in updating a counter.

When querying, you can easily project a counter value: `from Services select counter(Requests)`. But querying on a counter
value is a different story. Consider the following query: `from Services where counter(Requests) > 1000`.
RavenDB ensures that queries are fast by answering them from the index, always. But in order to query using a counter's 
value, we need to actually _index_ the counter value.

That is a problem, and not just because the previous statement is confusing. It is a problem because if the counter value
is has a high frequency of change, we'll be constantly re-indexing. We'll discuss the details of indexing and querying in
RavenDB later in this book, on Part III. 

#### How counters are implemented

RavenDB is capable of handling concurrent and distributed updates to counters, always converging on the real value. But
_how_ is it able to do that? You don't need to know this in order to make use of counters in RavenDB, but it might be a 
good idea to skim through this section to understand what is going behind the scenes.

Internally, each counter has the following structure: Node Id, Value, Etag. To be rather 
more exact, we have this structure for a counter for every node that has made a write on the counter. 

In other words, let's assume that we have a three node cluster (nodes A,B and C) and a counter that was modified on nodes 
A and B. The state of this counter (on all the nodes in the cluster) is shown in Table 5.1.

| Node | Value | Etag |
|------|-------|------|
| A    | 4     | 17   |
| B    | 3     | 22   |

Table: Internal state of a counter value

From the information in Table 5.1, we know that the final value of the counter is 7. If we'll increment the counter by 3 
on node A, RavenDB will update its internal state to be similar to Table 5.2, for a total value of 10.

| Node | Value | Etag |
|------|-------|------|
| A    | 7     | 18   |
| B    | 3     | 22   |

Table: Internal state of a counter value, after updating on A

The `Node` and `Value` are fairly obvious, but what is this `Etag` doing there? The `Etag` is used to reliably resolve
distributed updates. Consider the case of two updates, happening at the exact same time, to nodes A and B. We decrement
the value by 2 on A and increment by 3 on B. At this point, both nodes A and B will notify node C (and each other) about
their updates. Table 5.3 shows how the update looks like from the point of view of node C.

| Node | Value | Etag |   | Node | Value | Etag |
|------|-------|------|---|------|-------|------|
| A    | 7     | 18   |   | A    | 5     | 19   |
| B    | 6     | 23   |   | B    | 3     | 22   |

Table: Inconsistent counter state after distributed modifications

When node C gets both update notifications, it needs to resolve them to a final value. It is able to do that using a 
simple process. For each entry in the counter values, find the maximum `ETag` for a particular node and use that value. 
In other words, we'll end up with Table 5.4.

| Node | Value | Etag |
|------|-------|------|
| A    | 5     | 19   |
| B    | 6     | 23   |

Table: Counter state after resolving conflicted values

For Node A, we selected the `Value` (5) with the highest `Etag` (19) and for node B we selected the `Value` (6) for the 
highest `Etag` (23). The final result is 11, which is the correct answer (10 - 2 + 3).

In other words, for counter, we keep track of the values for each node it was modified on. That node is the owner of that
value and the only one that can change it. And whenever that node's value changes, we increment the `Etag`. So if we have
to select between multiple values, simple finding the entry with highest `Etag` for a node ensures that this is the latest
value that we got from that node (directly or indirectly) and we may then proceed finding the final tally.

In practice, you should rarely concern yourself with these implementation details. You can get the raw values from RavenDB
using `counterRaw()` instead of `counter` in your queries, but that is there mostly to be able to show what is under the 
covers, not for any expectations that this will be used directly.

##### Overflow handling

Counter values in RavenDB have the following range: `-9,223,372,036,854,775,808 .. 9,223,372,036,854,775,807`. Attempts to 
increase the value over the maximum or below the minimum will *not* overflow. An error will be raised in this case.

An exception to this overflow behavior happens when you are running in a distributed mode and make concurrent 
changes to your counters on distinct nodes. Consider the case where a counter is set to `9,223,372,036,854,775,800`.
In other words, just 7 values from the maximum value possible for 64 bits integer.

At the same time, we are going to add `5` to this counter on two separate nodes. Each one of those operations is 
valid on its own, but together, they'll push the value over the maximum allowed. Because they are done on independent
nodes, it is not possible to detect this in time. 

RavenDB will merge all those values and continue operations. The final value of the counter in this case will be set
to the maximum allowed to 64 bits integers. The reverse is true in the case of underflow, of course.

Note that the underlying values for each node are maintained, so decrementing the new value by `1` will not be
decrement the value. You'll need to decrement by `6` to see an actual change in the value. An example using 8 bits
integers will be easier to understand. Consider a a cluster with the state shown in Table 5.5.

| Node | Value |
|------|-------|
| A    | 59    |
| B    | 59    |
| C    |  6    |
|------|------ |
| Total| 124   |

Table: Distributed counter state, near the overflow limit

Adding `3` to both Node A and Node B at the same time will result in an overflow. The state of the counter after such
an addition and replication is shown in Table 5.6.

| Node | Value |
|------|-------|
| A    | 62    |
| B    | 62    |
| C    |  6    |
|------|------ |
| Total| 127   |


Table: Overflow on distributed counter peg the value to the maximum value

Remember, we are using an example with 8 bits numbers here, so the maximum value is 127, even though adding 
all the numbers will give us 130. 

Decrementing the value by `1`, however, will not reduce the total to 126. Instead, we'll have:

| Node | Value |
|------|-------|
| A    | 62    |
| B    | 62    |
| C    |  5    |
|------|------ |
| Total| 127   |

Table: Changes are allowed on distributed overflown counter

We reduced Node C by one, but the value still overflows. We'll need to decrement the value three more times
to see the total value drops. 

RavenDB uses 64 bits signed integers for counters, so the likelihood of an actual overflow or underflow is very 
low. I'm including this information here for completion's sake and so you can plan even for unlikely events.

You can do a lot with counters in RavenDB, but in the end, they are a very simple concept. A named `int64` value that you 
can manipulate concurrently and in a distributed fashion. RavenDB offers a far more sophisticated option for complex data, 
time series. Allowing you to track values over time.

### Time Series

Time series, like counters, offer a dedicated way to store and query specific types of data inside RavenDB. As 
we previously mentioned, storing such data inside the document is not an optimal option. Unlike counters, which are
fairly simple, time series offer a lot more capabilities and features. We'll start by first understanding what is
time series data in the first place.

As the name suggest, time series data is intrinsically tied to time. A good example of a time series is your heart rate. 
A single measurement of your heart rate isn't really that useful. What you typically want to see is your heart rate over
a period of time. You don't particularly care what is the heart rate value in a particular instant, but
the _trend_ over time is interesting. Time series are in incredibly common usage these days. Whatever this is tracking
sensor data from IoT devices, recording the exchange rate of crypto-currencies or measuring biometrics information for 
health care purposes. 

In the beginning of the chapter we looked into one way to model time series data in RavenDB, directly in the document 
itself. This _works_, but it isn't ideal. It is very common for time series to be _big_, not because the individual values
are big but because we track them over a long period of time.

Let's consider the heart rate example. I use a watch that measure my heart rate on a regular basis and I have been 
wearing it for the past 5 years or so. Assuming that the watch takes a measurement every 15 seconds, that gives us a total
of 10.5 _million_ data points. If we would try to put that into a single document, it would take over 350MB and would be
far beyond any reasonable size for a document.

We could try breaking it apart on some boundary (each week / month being a separate document would be a good start) but 
that isn't really the best solution for our purposes. Because time series are so common, RavenDB has dedicated support for
storing time series data. 
Time series do not exist independently inside of RavenDB but are associated with documents (just like counters and attachments).
A document may have a number of time series and a time series may have any number of entries, representing values over time.

Good examples of time series data are:

* Stock price for a company
* Heartrate of a person
* GPS location on a truck
* Temprature and humidity in a storehouse

The common thread across all of them is that you are usually more interested in how the value work over time, rather than
a specific value. Another issue that you have to take into consideration is that time series can be _large_. It isn't 
uncommon for a single time series to have millions of values. 

> **Time series terminology**
>
> It can be confusing to discuss time series, because a time series in casual speak may refer to all the values in the series
> or a particular value or range in it. To make things easier, I want to clearly define the terminology that I'm using
> in this chapter.
>
> * A `time series` is used to refer to a single time series on a document. 
> * A `time series entry` or `entry` are used interchangeably to refer to a single record `(time, tag, values)` 
>   inside a time series. 
> * `Values` or `value` are used interchangeably to refer to the _data_ that is being record. The heart rate, GPS location, etc.
>   The value(s) are always numeric (`double`, to be precise) and an entry can have 1 - 32 values associated with it.
> * `Timestamp` is used to refer to the time of a particular entry. Given how frequently we use the term `time` in this chapter
>   there was a need to have a word to unambiguously point to the time of a particular entry. `Timestamp` is always in UTC.
> * `Tag` is used to refer to a free text field that can be added to a time series entry.
>
> A single entry can have up to 32 numbers (double precision floating point) and a string tag. However, the tag and
> the values serve very different purposes. The values represent the specific numbers at the time the measurement was taken
> but the tag is usually used to store information about the measurement itself. For example, what is the source of the
> measurement, what is the confidence level in the numbers, etc.
>
> RavenDB is optimized to assume that tags between different timestamps are reused many times. Using a _unique_ value for
> the tag on each timestamp is going to be wasteful in memory and storage space and is not recommended.

RavenDB handles time series data in a similar manner as counters, by storing that in a dedicate model outside of the actual
documents. This allows us to apply a whole number of optimizations to the way we hold and process the data. But before we
dive too deeply into that, let's look into how time series are handled. We'll start with Figure 5.5, showing a document with
a single time series: `Location`.

![Document with time series data in the studio](./Ch05/img05.png)

Just like counters, a document with time series is flagged with that information and the list of all the time series for
a particular document is available in the `@timseries` array in the `@metadata` section of the document. To the side of
the document view in the studio, you can see some statistics about the time series. How many entries it contains
and the date range of the values. 

In this case, we are showing just a few entries, but we have tested RavenDB with time series containing hundreds of 
millions to billions of entries. RavenDB is able to handle them efficiently and without fuss. Later in this chapter
we'll discuss some of the implementation details that make such behavior possible. 

In the meantime, let's see what data we can store using time series. Figure 5.6 shows the actual time series entries.

![Time series entries for our office location over the years](./Ch05/img06.png)

There are a few interesting details to pay close attention to in FIgure 5.6. You can see that each entry in the table has
a time, a tag and the values. The time is fairly obvious, it is the time for the entry. And the values is also 
straightforward. Those are the latitude and longitude coordinates for our offices. We are using RavenDB's ability to 
store more than a single value in an entry. 
What remains a mystery is the tag, which isn't so obvious. In this case, the tag is used to denote the city our 
offices is located in. We'll see how tags can be useful later on in this chapter.

Working with time series data from the client is very similar to counters, as you can see in Listing 5.8.

```{caption="Recording your heartrate" .cs}
using (IDocumentSession session = store.OpenSession())
{
    Watch watch = GetDevice("fitbit");

    (DateTime time, double bpm) = watch.GetLatest();

    session.TimeSeriesFor("users/oren", "Heartrate")
        .Append(timeToUniversalTime(), bpm, "watches/fitbit");

    session.SaveChanges();
}
```

The code in Listing 5.8 grabs some data from a device (a FitBit, in this case) and record it in the appropriate time series. 
There are a few things to note about the code in Listing 5.8. In order to work with time series, we use the `TimeSeriesFor()`
method, providing it with the document id and the time series name we want to work with. We can then `Append` an entry to 
the time series. 

Just like with counters, appending an item to a time series will create it and removing the last value from a time series will
remove it. There is no need to create or delete time series explicitly. Concurrent distributed operations on a time series will
be automatically merged. In the case that two entries for the same time on the same time series exists with conflicting values
RavenDB will select the entry with the highest value and use that going forward. 

#### Considerations of time series usage

To simplify matters, RavenDB uses UTC dates only for time series, you don't need to convert to and from the server's time zone.
When appending a value, you provide the time (in UTC), the value(s) and the tag. RavenDB supports 1 - 32 values per time 
series entry. Time series values are a double precision floating point numbers (or just `double` in your programming language
of choice). Note that the `NaN` value is excluded from the possible values (but `Infinity` is allowed). 

It is preferred, but not required, to append to the time series in a sequential fashion, as that result in the optimal layout
of data on disk. If you write to the time series out of order, RavenDB has to do a bit of extra work, but everything is going
to work as you expect it to. The granularity of the time series's timestamps is in millisecond. Writing to a value with a 
timestamp that already exists in the server will overwrite that value. 

In other words, if you are writing data from sensors or recording data on the fly, you are already going to be doing 
the Right Thing, as far as RavenDB is concerned. Always appending data at the end and appending data in a sequential manner.
If you don't match that behavior, if you need to write out of order or update values, that is fine. RavenDB will just make 
things work for you. 

In the same manner, RavenDB will work best when all the entries in a time series have the same number of values. However,
you are free to have entries with different number of values, if that make sense for your scenario. The number of values
of a time series entry is in the range of 1 to 32. You can treat these values together or as independent values. 

For example, when recording GPS coordinates, there is little sense in splitting the latitude and longitude values. You can't really use
just one of them. The same is the case when you record blood pressure measurements, with systolic and a-systolic values that
go together with one another. On the other hand, if you are recording currency exchange rates, you might record every minute
the exchange rate of multiple independent currencies. In such a case, you might create separate time series for each currency pair
instead of putting all the rates in a single time series. We'll cover modeling concerns for time series later in the chapter.

If you are appending to a time series with variant number of values, you need to be aware that each time series entry accepts
an array of `double` values. In other words, you are free to append an entry with 3 values and then an entry with 2 values, but
the _position_ of the value matter. There are no named values in time series, and you cannot pass a sparse array to the append
call. 

If you have values that might be missing for some entries, make sure that you place them toward the end of the values array,
or consider using multiple time series to record such data. Remember that like any other feature in RavenDB, you can safely
mix time series operations with other operations safely in a transaction. In other words, you can have a transaction that would
atomically append values to two time series (on the same document or multiple documents) with no hassle.

#### Tags in time series entries

You might have noticed the notion of a `tag` that keeps popping up when discussing time series data in RavenDB. What is
that about?

A time series entry is composed of a timestamp and a set of values that represent measurements taken at the specified time.
In a perfect world, this would be all that we need to deal with the data. Indeed, a lot of complexities that are usually
associated with time series are handled by the association of time series to documents. We'll touch on this more later
in this chapter, when we'll talk about modeling concerns for time series.

Consider the case of heartrate monitoring. There are many ways to monitor one's heartrate. For example, I'm currently 
wearing one such device, a FitBit. At some other time, I may wear an Apple's Watch or be hooked up to a medical device
during a routine checkup. 

In other words, even though the values go to the same place and have roughly the same meaning, there is a lot of value
in recording additional information about the recording itself. It _matters_ if I'm using a consumer device or a hospital-grade 
system, and we should be able to record that. That is where tags come into play. 

Tags allow you to provide additional meta information about the recorded values. Typically, this is some information about
how the information was achieved. When recording GPS data, we'll record the device model that was used to get the 
coordinates. If I'm keeping track of multiple exchanges watch for currency fluctuations, I might want to record which 
exchange a particular rate applies to, etc.

Tags allow you to tag a single entry with a string (up to 255 bytes). Because the size of the data is limited, you'll 
usually not store the data directly inside the tag but put it in a document and store that document id in the tag.
For example, in the GPS tracking example, you'll store the device's document id as a tag, and then have a document 
describing the capabilities of the device. 

That raise a question, why would we create a document for each time series entry? The whole _point_ of time series is to
allow you to store vast amount of data efficiently. If we create a document per entry, that isn't very efficient, is it?
The answer is that is that you aren't going to have a document per time series entry. Indeed, RavenDB optimize heavily
toward _repeating_ tags values.

The idea is that the time series entries are going to repeat the tag value frequently. Using the heartrate example, most
of the time, you are going to get values from a single device. In Listing 5.8, the tag was `watches/fitbit`, for example,
and it is safe to assume that most of the entries are going to share that tag. Changing the tag isn't an issue, either, 
because then you are going to repeat the new tag often.

> **Tags are metadata _pointers_ about the entry**
>
> It's easiest to think about tags as a way to add a reference for additional information about the entry. A document id
> is a great example of how you can store additional data about your time series entries. 
>
> RavenDB anticipates such usage and have several features around querying of time series that assume that you can fetch
> additional information about a specific entry from the document referenced by the entry's tag.

With the current exchange rate example, you'll tag each entry with the document id of the exchange we got the values from.
In this case, we will have interleaved tags. Each time that we append an entry to the time series, it may come from another
exchange. But in that case, you'll have a (small) set of values that are repeating, which is also something that RavenDB
is well prepared for.

This is a heavily optimized convention, not a requirement. You can chose to omit the tag entirely, if it make no sense in
your scenario. There is nothing to be concerned about by omitting the tags. On the other hand, adding many distinct 
tag values will _work_, but prevent RavenDB from optimizing storage usage properly. 
See later in the chapter when I discuss the implementation details of time series.

#### Reading time series data

I spent a lot of time discussing appending data to a time series, but I haven't actually gotten around to showing how we
can make use of time series data. This isn't an accident, for the most part, you'll write a lot more than read time series
data. In fact, reading time series data is almost always done using queries, aggregating the information to specified date
ranges. 

For example: "show me my average heart rate every 5 minutes for the past week". We haven't really touched on anything but
the simplest queries yet, but we'll briefly touch the topic now, just to show what is possible. Expect further discussion 
on time series queries in Part III of the book.

Listing 5.9 shows how you can ask RavenDB to give you the raw values from the time series for a specified date range.

```{caption="Read raw time series values for the last week" .cs}
using (IDocumentSession session = store.OpenSession())
{
    var from = DateTime.Today.AddDays(-7).ToUniversalTime();
    var to = DateTime.Today.ToUniversalTime();
    IEnumerable<TimeSeriesEntry> entries =  
        session.TimeSeriesFor("users/oren", "Heartrate")
         .Get(from, to);

    foreach (var entry in entries)
    {
        // entry.Timestamp,
        // entry.Tag, 
        // entry.Values
    }
}
```

In Listing 5.9, you can see how we call the `Get()` method to fetch all time series entries in the specified date range. The
result of the call is an enumerable of all the raw entries in the range. Each of the entries contains the timestamp, the tag
and the values that were recorded. 

Just like with counters, if you know that you are going to need a time series range upfront, you can `include` that when 
you load the document, saving a remote server call. You can see how this is done in Listing 5.10. 

```{caption="Including a time series range when loading a document" .cs}
using (IDocumentSession session = store.OpenSession())
{
    var from = DateTime.Today.AddDays(-7).ToUniversalTime();
    var to = DateTime.Today.ToUniversalTime();

    User user = session.Load<User>("users/oren",
        i => i.IncludeTimeSeries("Heartrate", from, to));

    // This does not send a server request, data is 
    // already loaded into the session
    IEnumerable<TimeSeriesEntry> entries =
        session.TimeSeriesFor("users/oren", "Heartrate")
            .Get(from, to);
}
```

The code in Listing 5.10 uses the same date ranges for the `Load()` and `Get()` calls, but the RavenDB API allows you to 
include a date range and then call `Get()` with smaller date ranges. So you aren't required to use the exact date range when
calling `Get()`. Note that if you `include` a range, and then `Get()` a range that exceeds the required range, the RavenDB 
client API will send a request to the server for the parts of the range that it is missing. The code will work, but will 
generate an additional remote call.

One thing to note about `include` and time series, you have to be careful of including time series data over date 
ranges that may contain a _lot_ of entries. For example, if I'm asking for the heartrate results for the last week, and I'm 
recording a heart rate every 5 seconds, then I'm going to get back `120,960` results. That is going to consume network bandwidth
as well as server and client side memory. And you aren't likely to be _interested_ in that kind of detail.

You want to get some aggregation of the measurements. Listing 5.11 shows how this can be done through the client API.

```{caption="Querying user's heartbeat, avg and max heart every 5 minutes for the past week" .cs}
using (var session = store.OpenSession())
{
    var from = DateTime.Today.AddDays(-7).ToUniversalTime();
    var to = DateTime.Today.ToUniversalTime();

    List<TimeSeriesAggregationResult> results = 
        session.Query<User>()
            .Where(u => u.Id == "users/oren")
            .Select(u => RavenQuery.TimeSeries(u, "Heartrate", from, to)
                .GroupBy(gb => gb.Minutes(5))
                .Select(g => new {Avg = g.Average(), Max = g.Max()})
            .ToList()
        ).ToList();
}
```

The output of the code in Listing 5.11 is going 84 entries, one per each 5 minutes range in the past week, with the average
and maximum value recorded in that time frame. RavenDB's query engine is quite flexible and allows you to run a whole host
of interesting queries on time series, including grouping, aggregation, filtering and projection. We'll cover these in depth
in Part III of this book. 

> **You get what you put**
> 
> One thing that I want to note here, however, is that we'll get an entry per each 5 minutes range _for which we have data_. If 
> the user didn't wear the watch at night, for example, we'll not get any entries for that time frame. 
>
> In other words, RavenDB is not going to inject empty values for missing ranges. You can do that, if you want to, but we'll 
> discuss such features when we go into time series queries in depth, in Part III of the book.

Time series data is everywhere, and as it turns out, there is quite a lot of it. If you keep track of a truck's location every
5 seconds for a year, you'll end up with `6,307,200` entries, for example. If you keep track of a value every second, after a
year, you'll have `31,536,000` entries to deal with. RavenDB is _very_ efficient in how it stores time series data, as you
can see in the section discussion time series implementation later in this chapter. Sometimes, however, it is best to just not
store the data, or store it already aggregated. This is where roll-up and retention policies come into play.

> **Reading large amount of time series data, anyway**
>
> I mentioned earlier that reading a large number of time series entries is not advisable, because it forces a _lot_ of data
> to be loaded upfront. This is true, but there are several scenarios where you _need_ to do so. For example, imagine that you
> have a large time series and you want to export the raw results to a CSV file.
>
> In this case, regardless of the data size, you still need to access all the results. You can do that using the usual API, but
> it would force materialization of the values in memory. A better approach is to use streaming queries, as we discussed in 
> Chapter 4. That allows you to process large amount of data without having to keep it all in memory.

#### Roll-up and retention policies

Consider the case of a fleet of ten thousand trucks, which we keep track off using GPS every 5 seconds. After a year, you are
going to have 63 _billion_ entries to deal with. It is unlikely that you care about each truck's position in that level of
granularity. RavenDB time series allow you to define policies (in the Studio, go to `Settings > Time Series`) for how RavenDB
should treat your time series data.

You can see an example of such a policy in Figure 5.7. The scenario we have is keeping track of trucks' location, where we have
an entry per 5 seconds. The data really piles up quickly in this scenario, but we don't actually care about it to that level
of granularity, so we can define a roll-up policy, as you can see in Figure 5.7.

![Configuring roll-up and retention policies for Trucks' time series](./Ch05/img07.png)

There is a lot going on in Figure 5.7. First, we define the `retention` for the _raw_ time series data on the `Trucks` 
collection. In this case, we want to keep the raw data (every 5 seconds) for a month. The next stage we see in Figure 5.7 is
the `roll-up` by 10 minutes. We want to roll the data from 5 seconds sample rate to 10 minutes sample rate. That level of data
we keep for 2 months, and then we just keep track of the location on an hourly basis forever.

The notion of retention is fairly simple, we tell RavenDB to simply discard data that is older than the specified period. But
what does _roll-up_ mean? In the case of taking 5 seconds sample rate and translating that to a 10 minutes sample rate, for 
example. What happens to the data?

RavenDB aggregates it in the relevant time frame and generate 6 aggregates for each value that you have in the original time
series. These values are:

* First
* Last
* Min
* Max
* Sum
* Count

In the case of the `Trucks`' GPS data, we actually have _two_ values. The latitude and longitude coordinates of the truck's position. And
each 10 minute period contains 120 entries detailing the location of the truck every 5 seconds. The roll-up of the `Location` 
time series will be written to `Location_ByTenMinutes` as 12 values. The first six values represent the aggregation on the 
latitude and the last six represent the aggregation on the longitude. In other words, each of the values in the original
time series is rolled up independently. 

> **Roll-ups are limited to timeseries of up to five values**
>
> Roll ups generate six aggregated values for each one of the original values in the time series. The rolled up time series
> is limited to 32 values, which means that at most you can roll up a time series of five values only.
>
> In the rare case that you need roll ups on such a scenario, you can either split the time series to 
> multiple independent series mirroring one another or do the roll-up manually. 

Rolling up across multiple aggregate functions has some interesting implications. You don't need to define what you want up
front, and can usually just let RavenDB handle things for you. If you later find that you need an aggregate, you haven't missed
up on the data that was already rolled up. You'll also note that a very important aggregation is missing here, the average. It
is excluded because you can safely compute it from the `Sum / Count` aggregation. 

The rolled up data is a time series in its own right. In other words, you can do all the usual things you can do to a 
time series. Query it, read from it and even _write_ to it. I recommend avoiding this, but it is possible (mostly to allow
to import already rolled up data from other sources).

You'll notice in Figure 5.7 that we have defined multiple roll up policies. First by 10 minutes and then by an hour. It is 
important to understand that such policies are built on one another. In other words, the data source for `Location_ByOneHour`
is the `Location_ByTenMinutes` time series. This does raise an interesting question, however, _when_ would RavenDB perform
the actual roll-up?

The answer to that is that it will perform a roll-up every time that the roll-up period roll over. In other words, for the 
`Location_ByTenMinutes`, we'll do a roll-up every 10 minutes. For the `Location_ByOneHour`, we'll do a roll-up every hour. We 
don't wait for the retention period to lapse before we do the roll-up. 

> **Transparent queries across roll-ups**
>
> When you query a time series, you will typically query the actual timeseries, and not one of its roll-ups. In other words,
> your query will focus on `Location`, and not `Location_ByOneHour`. If you have query extends beyond the retention limit for
> the time series in question (1 month for `Location`), RavenDB will transparently use the roll-up time series to answer your
> query.
>
> If you want to get the average of the location of a truck in one hour increments, across a whole year, RavenDB
> will get the data for first month from `Location`, and then aggregate the data from `Location_ByTenMinutes` for the 
> next month and the rest of the year will be computed from `Location_ByOneHour`. 
>
> You don't need to do anything special to make this happen, RavenDB will get the right results for you.
>
> This feature allow the administrator to define roll-up policies after the fact, without requiring application changes or
> new queries to be written.

Using the `Location` time series as an example, let's see what kind of results we get from rolling up the data. We no longer
have detailed information about the truck's location. We can tell exactly where it was every 10 minutes to an hour (depending
on how far back we go). We can do that by looking at the `First` value for the result. Listing 5.12 shows how we can output
the location over 10 minutes period. 

```{caption="Querying truck's location, including from the rolled up time series" .cs}
using (var session = store.OpenSession())
{
    var from = DateTime.Today.AddMonths(-2).ToUniversalTime();
    var to = DateTime.Today.ToUniversalTime();

    TimeSeriesAggregationResult result = session.Query<Truck>()
        .Where(u => u.Id == "trucks/TS-81-922")
        .Select(u => RavenQuery.TimeSeries(u, "Location", from, to)
            .GroupBy(gb => gb.Minutes(10))
            .Select(g => new
            {
                First = g.First(),
                Min  = g.Min(),
                Max = g.Max(),
            })
            .ToList()
        ).First();

    foreach (TimeSeriesRangeAggregation agg  in result.Results)
    {
        Console.WriteLine($"Located at {agg.From} " + 
                          $"in {agg.First[0]}/{agg.First[1]}"
        );
    }
}

```

Listing 5.12 shows how we can aggregate multiple values. In this case, the `First[0]` is the first latitude value and `First[1]` is
the first longitude value. We can use these to find out the location of the track every 10 minutes. Another possibility you have here
is to compute the bounding rectangle for the truck. In other words, in that 10 minutes time period, where was it according to
the actual coordinates we recorded?

We can do this using the `Min` and `Max` values, which represent the bounding rectangle of the location of the truck during 
that time period. In other words: `var boundingRect = new Rectangle(agg.Min[0], agg.Min[1], agg.Max[0], agg.Max[1])`. We can
be sure that _all_ the points in the time period are located inside this rectangle. 

I'm explicitly mentioning this techniques, the use of the `First` and the bounding rectangle to explain common scenarios of
aggregation of data across multiple values in a single time series. This does raise an interesting question, when should we
use a single time series to record values and when should we use multiple time series? Luckily, this is what I'll be answering
in the next section.

#### Modeling concerns with time series

What kind of data modeling _can_ we do with time series? The data format is simple, after all. We just have a series of these
entries: `( timestamp: DateTime, tag: string, values: double[] )`. RavenDB's time series have a few aspects that you might want
to consider when building time series data. 

The very first thing you need to consider when deciding how to use time series in RavenDB is what document the time series
is going to be attached to. The expected usage model for time series is that you'll have a time series on the document that 
provide additional context for the data. For example, `Heartrate` time series on a `User` document, a `Location` time 
series on a `Truck` document or `CPU utilization` time series on a `Server` document.

In the cases I mentioned previously, it is very easy to decide where to place the time series. But there are other cases where
the choice isn't so clear. Consider the case of tracking currency exchange rate. Where are you going to put the data? 
Consider the example shown in Figure 5.8.

![Three different ways to model Bitcoin to USD exchange rate](./Ch05/img08.png)

Figure 5.8 shows three different ways to model a time series of Bitcoin to USD prices. One option is to use `Exchanges/Coinbase`
document, where the document serves to hold time series per currency. Another option is to use `Currencies/Bitcoin` and a 
time series per corresponding currency (`USD`, in this example) with the tag used to denote the source of the data. The final
option is `Currencies/Bitcoin`, but a separate time series for each exchange (`Coinbase` time series, in this case).

The answer about the best model for the job is, unfortunately: "it depends". RavenDB is able to store the data using any of these
methods and has the facilities to perform the need computations and aggregations from any of them. 
The primary guiding light when deciding which model to chose is what is the primary use of the data. In the case of Figure 5.8, 
if we care about the exchange rate mostly within the context of a particular exchange, it is probably best to model that explicitly. 

If we mostly care about the actual exchange rate of the currency compared to other currencies, then having a `USD` 
time series on `Currencies/Bitcoin` is ideal. In such a case, the `Coinbase` tag is used for secondary purpose and may be useful
for some needs, but we mostly focus on the actual values we have. 

Finally, if what we care about is comparing the data between exchanges (for example, to find arbitrage opportunities), having
`Currencies/Bitcoin` with a time series per exchange makes the most sense. We can then compare between them and see if there
are options that we want to take advantage of. In this case, most of the work is done by _comparing_ time series to one another.
It is best to model such a need explicitly. 

The next topic should be the tagging strategy for the data. RavenDB allows you to associate a tag with each entry, which 
allow you to provide additional context about the entry. These tags are not meant to be free form text, however. They are
meant to be a small set of possible values for an entry. The most common use case for them is to contain a reference for
a document. You can see an example of such tags in Figure 5.9.

![Using tags to denote the source of the entry's data in the series](./Ch05/img09.png)

In Figure 5.9, we use the tags to allow us to know what the source of the data is. In the case of `Heartrate`, it can be
very useful to know the specs of the device taking the measurement, because that may tell you how reliable it is. While
there is no _requirement_ that the tag will point to a document id, I'll strong recommend it be the case. RavenDB has several 
features that allow you to take advantage of document ids as tags. An example of which is in Listing 5.13.

```{caption="Querying time series data, filtering by properties of the tagged document" .cs}
session.Query<User>()
    .Where(u => u.Id == "users/oren")
    .Select(u => RavenQuery.TimeSeries(u, "Heartrate")
        .LoadTag<Watch>()
        .Where((ts, watch) => wathc.Accuracy > 0.75)
        .GroupBy(gb => gb.Hours(1))
        .Select(g => new
        {
            Avg = g.Average(),
            Max = g.Max()
        })
        .ToList());
```

In Listing 5.13, you can see the `LoadTag<Watch>()` call, this tells RavenDB to treat the tag as a document id and fetch the 
referenced document. We can then filter our queries by using the properties of that document. We'll learn why you want to have
just a few distinct tag values in the implementation section later in this chapter.

> **The tag's referenced document**
>
> When you `append` an entry with a document id tag, there are actually _two_ documents that are being referenced by the
> time series entry. The first (and most important) document is the document that owns the time series in question. The
> second document is the one pointed to by the tag. 
>
> The distinction is important. The owner document is the one the time series is _about_, the referenced document from the
> tag provides _secondary_ information about the entry (accuracy, location, model, etc). 

A tag allows you to reference from a time series entry to a document. Sometimes, you need to go the other way around. For 
example, in an automated ticketing system, you'll have a document per traffic camera, with a time series recording the speed
tracked by the camera. In this case, a ticket needs to refer to a specific time series entry. How can we do such a thing?

There is one way to locate a specific time series entry in a time series, using it's `timestamp`. And locating a time series
required its document id and name. In other words, our `Ticket` document will include the following triad:
`["cameras/3449-C", "Speed", "2020-05-12 12:33:04.000Z"]`, allowing us to identify the exact entry for the ticket. Situations
where you actually need to do that are quite rare, however.

The last topic that I want to discuss about modeling is the decision to use many documents with a single time series each or 
one document with many time series. The same principles that affect counters are in force here as well. A document has a list
of all the timeseries it has in its metadata. If you have _many_ such time series, you may end up with a document whose metadata
is significantly bigger than its content. That is a _rare_ scenario to happen by intent, but if you have a model in which
you add time series to a document dynamically, you might want to pay attention to the growth scenario.

Aside from putting a _lot_ of time series on a single document (and by a lot, I mean in the thousands), the primary goal that 
should drive you toward selecting the right document for a time series is the association. It make sense to put data that is 
used often in the same location. Note that this isn't something that RavenDB desire for optimization purposes. Time series and
documents are stored separately, but because it make sense to _humans_. It means that you can find all the relevant information
about an entity in your domain from a single location. We have spoken about that extensively in Chapter 3, and it is as true
for time series modeling (as well as counters) as it is for data you put in the document itself. 

#### Working with typed time series data

So far in this chapter, we have mostly worked with the untyped time series API, using array of doubles and a specific position
find an appropriate value. If you are working with a time series containing a single value, such as Heartrate, that is not an 
issue. But when working on complex time series with multiple values, this is awkward. Consider the case of the Truck's driving
information. Each time that we record an entry, we have: `Latitude`, `Longitude`, `Direction` and `Speed`.

We can work with the data as an array of `double`, assigning meaning to each position, but that is awkward to do. RavenDB has
better options available. Listing 5.14 shows how you can use strongly typed time series with RavenDB.

```{caption="Defining a class to allow strongly typed work with time series data" .cs}
public class TruckTripEntry
{
    [TimeSeriesValue(index: 0)] public double Latitude;
    [TimeSeriesValue(index: 1)] public double Longitude;
    [TimeSeriesValue(index: 2)] public double Direction;
    [TimeSeriesValue(index: 3)] public double Speed;
}
```

The class shown in Listing 5.14 allow you to utilizing time series using strong typed values. The `TimeSeriesValue` attribute maps
the value's index to the appropriate field or property. Listing 5.15 shows how you can make use of this API.

```{caption="Using strongly typed API with time series" .cs}
IDocumentStore store = GetDocumentStore();

// Register the names with the database, so we can use them 
// on the server side as well
store.TimeSeries.Register<Truck, TruckTripEntry>();

using (var session = store.OpenSession())
{
    session.TimeSeriesFor<TruckTripEntry>("trucks/1293-A")
            .Append(DateTime.UtcNow, new TruckTripEntry
            {
                Latitude = lat,
                Longitude = lng,
                Directory = direction,
                Speed = speed
            });

    session.SaveChanges();
}

using (var session = store.OpenSession())
{
    foreach (TruckTripEntry entry = 
        session.TimeSeriesFor<TruckTripEntry>("trucks/1293-A")
            .Get(DateTime.Today.AddDays(-7),DateTime.Today))
    {
        // iterate over strongly typed entries
        // for this truck for the past week
    }
}
```

Listing 5.15 shows two important aspects of using the strongly typed API. First, we have the `Register` call, which registers the
fields of the `TruckTripEntry` class on the `Truck` collection. This allows server side usage of named time series fields, as well
as provides the appropriate labels for the values in the Studio. 

The other interesting API call in Listing 5.15 is the usage of `TimeSeriesFor<TruckTripEntry>()`, which allows us to use strongly
typed class for appending time series data, saving us the need of passing raw array. You can also use it to get typed results 
from the server.
You can read all about working with strongly typed values with time series in the online documentation.

#### Time series implementation in depth

We covered a lot about how to _use_  time series in RavenDB, but I wanted to dive just a little bit into the implementation
details to give you some context on how to best utilize time series. RavenDB stores the time series for a document independently
from the document itself. The actual data is stored in segments, which can fit some amount of data. Each segment contains the
entries for a particular date range. A time series is effectively a list of these segments.

The segmentation of the time series is a key concept to the implementation of time series in RavenDB. This is because all the
data in a segment is treated together. For example, I mentioned a few times that the `tag` value is expected to be part of a 
small set of values. This is true, but only on a single segment level. In other words, if you start up using one tag and then
switch to another, and then another and so on, that is going to be fine. Each one of those will end up with its own segments
and have a small set of values that it can safely optimize. 

A time series segment has the following limits:

* The maximum size a segment can reach is 2 KB.
* The maximum date range is about 24.8 days.
* The maximum number of entries is about 32,000. 

In practice, a typical segment will contain 200 to 4,000 entries, depending on a lot of factors. Each segment is composed of
a start date, a header and the set of entries in the specified range. A time series segment is typically appended to on the 
end, and it is heavily optimized for that purpose. A write to the middle of a segment will force RavenDB to split the 
segment into two pieces. That is one of the reasons that appending to the end and in a sequential manner is best for RavenDB.
Writing randomly works, but require us to fragment the segments or rebuild them, which takes more work.

One of the ways that RavenDB improve time series performance is by aggregating everything on the segment level on append. 
In other words, on a per segment basis, we store some aggregated results in the segment's header. These are `First`, 
`Last`, `Min`, `Max`, `Sum`, `Count` and `Average`. These are the same aggregation that we get when we do roll-ups, and they are 
incredibly common operations. By doing the work on a per segment basis at write time, RavenDB is able to achieve very 
fast query performance over wide range of data. You can see how the data is structured in Figure 5.10.

![Physical outline of a time series segment](./Ch05/img10.png)

The actual data for the time series entry is stored using a format called Gorilla 
compression^[The paper describing this compression scheme is here: http://www.vldb.org/pvldb/vol8/p1816-teller.pdf ]. 
This is a common format for storing time series data and it takes advantage of some of the commonalities in time series data. 
In particular, if the rate you record the data is the same, and the data doesn't change significantly, RavenDB is able to 
dramatically reduce the amount of space an entry consumes. Assuming optimal settings, we can get away with recording an entry
with as few as four bits (in other words, half a byte). 

To make things even more efficient, space wise, once a segment is full using Gorilla compression, we'll compress it again, 
using the Lz4 algorithm. The end result is extremely space efficient. At the same time, because the common aggregates are 
stored in the header (and not compressed), RavenDB it able to access them very cheaply for common queries.

> **The size of the segment's header**
>
> A segment size is limited to 2KB, most of which is expected to be filled with entries' data. However, each segment also
> contains some aggregated information about each of its values (last, first, min, max, sum and count). These values cannot
> be compressed and consume a total of 48 bytes per value.
>
> If a segment has the maximum number of values (32), each of them will have six aggregated values to track and the size of
> that alone will be 1,536 bytes, leaving under 512 bytes for actual entries data. For that reason, even though you _can_
> store large number of values in a time series, you might want to split it to different time series. The space savings 
> would be significant.

Splitting the data into segment also helps RavenDB in a few other cases. Replication of time series data is handled at the
segment level. Instead of sending individual writes to other nodes, RavenDB will send the full segment. Given that a segment's
maximum size is 2KB, that isn't really an issue and usually allows us to batch many changes in one server round trip.

Indexing of time series is also done at the segment's level, with the ability to drill down into the individual entries. We'll
discuss such indexes in Part III of this book.

I doubt that you'll ever really need to concern yourself with how the actual segments work inside of RavenDB. You don't 
really have control over how RavenDB manage it segments, but the information about how it works behind the scenes can be 
useful. In particular, if you can make your data predictable, that can increase the compression rate significantly. 

Your data is predictable if you record measurements at the same distance from one another, keep the values to meaningful 
significant digits, etc. If the data is repeating, that would generate the best possible outcome. For example, tracking
a truck's location every second for 48 hours while it is parked over the weekend can fit in 120 KB or so. That may not sound
impressive, until you realize that we are talking about 172,000 values that are compressed to that level. 

On the other hand, keep in mind that such optimization might not make sense for your scenario. If you are keeping track of 
time series with a period of fifteen minutes, you are likely to hit the date range limit of 24.8 days faster than hitting the 
size limit on a segment. 

Testing with real world data shows that RavenDB is able to pack a single time series with 148,079 items into about 1.55MB 
(with a density of about 290 entries per segment).
The data in question represents GPS tracking data of a single taxi in Beijing^[The data was taken from the T-Drive 
data set, available: https://www.microsoft.com/en-us/research/publication/t-drive-driving-directions-based-on-taxi-trajectories/ ]. 
The entire T-Drive data set includes 10,357 taxis and a total of 15 millions points fits in 256 MB.

On the other hand, a time series with stock prices per business day for 55 years (around 14,000 entries) took 848 KB to store
(with a density of about 18 entries per segment).
The reason for the difference is that in a time series that span this range, we have to create separate segments per 24.8 days
and weren't able to store the data in as dense a manner. 

### Summary

In this chapter, we covered some of the ways RavenDB allows you to specialize the kind of data you store on documents. Counters
and time series provide ways to handle specific data types and operations. 

Counters allow you to handle high frequency counters, avoiding the problem of a single hot spot slowing your system down and 
providing easy path for handling a counter that can be modified concurrently and safely in a distributed environment. Modifying
the counter does not modify the associated document, and the underlying structure is friendly toward high number of writes and
allow merging of updates from any number of nodes seamlessly. 

Counters are typically used to handle page counters, likes on a post, compute rate limits or to compute usage statistics. They 
are inherently limited in what they can do (they are just an `int64`, after all) but can be very powerful when utilized properly. 
We also discussed data modeling with counters, using the counter names to keep track of different values. We used the example of 
`status/500` and `status/200` to keep track of the error rate of a particular service. 

In order to enable most of their advantages, counters are inherently eventually consistent. An `Increment` operation will return the
new value of the counter, but not the _decisive_ value. Other increments may have happened on other nodes at the same time. For most
of the use cases for counters, this doesn't matter, but it is worth noting this behavior. 

We closed the counters discussion with an in depth dive into how they are implemented behind the scenes and how that allows us to safely
merge updates from multiple nodes at the same time.

After covering all aspects of counters, we moved to discussing time series, which allow you to store data over time. Like counters, time 
series are associated with a parent document and uses a dedicated format to provide the most compressed storage model as well as 
fast queries over the time series data.

Time series are used heavily in the realm of IoT^[Internet of Things] and for tracking anything that changes over time. Health care data
such as heart rate or blood pressure, location data via GPS trackers, stock prices and currency exchange rates and many more scenarios
are much simpler when you model them explicitly as a time series.

We looked into modeling your data with time series, how should you structure your time series to fit your needs and the roles of the 
entry's tag in providing additional context to the time series entires. We closed the discussion on time series with a dive into how
time series are implemented by RavenDB and the implications for your use of time series with RavenDB. 

We have touched on counters and time series separately, but they are orthogonal features. There is no issue with using both counters 
and time series on the same document. The one thing to note is that the `@flags` property will be set to `"HasCounters, HasTimeSeries"`
in such a case.  Counters and time series both participate in transactions inside RavenDB. You can modify a document, increment a 
counter and append to a time series in a single transaction, and all these changes will be applied atomically or not at all. 

Counters and time series can provide you with a way to deal with data that doesn't fit naturally into the document model. It is an 
important corner stone of RavenDB's multi-model strategy and can make your life much easier when used properly. 

The next step is to learn what kind of usage we can make of the data inside RavenDB. The next chapter will deal with data subscriptions 
and all the myriad ways they make data processing tasks easier for you.